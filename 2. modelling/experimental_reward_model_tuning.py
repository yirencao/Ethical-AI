# -*- coding: utf-8 -*-
"""PM-tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uxt8JoeHRU85JtKlmcVeFZeQ0Q2qmX3B
"""

# Commented out IPython magic to ensure Python compatibility.
# Commented out IPython magic to ensure Python compatibility.
# %pip install -q datasets scikit-learn sentencepiece
# %pip install -q sentencepiece

# git+https://github.com/bigscience-workshop/petals@main

import os
import csv
# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'


import torch
import torch.nn as nn
import torch.nn.functional as F
import transformers
# import wandb
from datasets import load_dataset, load_metric
from tqdm import tqdm
from torch.optim import AdamW
from torch.utils.data import DataLoader
from transformers import LlamaTokenizer, get_scheduler, set_seed

# from petals import DistributedLlamaForSequenceClassification

set_seed(0)
from huggingface_hub import login
login()

# -*- coding: utf-8 -*-
"""7 Dec: prompt-tuning-sst2 (train PM).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EGArTWFvijr_Ixh2DnLdlzmpRKVN9-gT

<div>
<img src="https://camo.githubusercontent.com/473dd9f992924d27457650251786464f72e54121ac6e9210add0f483ca849277/68747470733a2f2f692e696d6775722e636f6d2f3765523750616e2e706e67" width="40%">
</div>

# Distributed LLaMA for Text Classification using Prompt Tuning

In this example, we show how to use [prompt tuning](https://aclanthology.org/2021.emnlp-main.243.pdf) to adapt the [LLaMA](https://github.com/facebookresearch/llama) model for a specific downstream task. We will run this model in a decentralized fashion using [Petals](https://github.com/bigscience-workshop/petals). Petals servers will maintain the LLaMA blocks (they are kept unchanged during adaptation), and the gradient descent will learn a few prefix tokens stored on a Petals client.

We will adapt LLaMA for the classification task using the [SST-2 dataset](https://nlp.stanford.edu/sentiment/). This dataset is a binary classification task, where the goal is to predict whether a sentence is positive or negative. The SST-2 dataset is a subset of the Stanford Sentiment Treebank, and it is available in the [Hugging Face Datasets](https://huggingface.co/datasets) library.

To use this notebook in Colab:

1. Follow this link: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb)
2. Go to **Runtime** -> **Change runtime type** and select the GPU accelerator.

First, we have to prepare all dependencies.
"""



"""Let's set some hyperparameters for training:"""

MODEL_NAME = "Enoch/llama-7b-hf"

# Choose a prompt-tuning mode ('ptune' or 'deep_ptune').
# The latter fine-tunes separate prefixes for each transformer block,
# so prompt-tuning will take more time but yield better results.
# See this paper for details of how it works: https://arxiv.org/pdf/2110.07602.pdf
TUNING_MODE = 'ptune'

NUM_PREFIX_TOKENS = 8
DEVICE = 'cuda'
BATCH_SIZE = 1
LR = 1e-2
WEIGHT_DECAY = 0.0
NUM_EPOCHS = 100
SEED = 42
MODEL_MAX_LENGTH = 64

"""Here, we prepare tokenizer and distributed model and connect it to the public swarm."""

tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)
tokenizer.padding_side = 'right'
tokenizer.model_max_length = MODEL_MAX_LENGTH
tokenizer.pad_token = tokenizer.unk_token

# Instead of using DistributedLlamaForSequenceClassification, use the local model
model = transformers.AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)


# model = DistributedLlamaForSequenceClassification.from_pretrained(
#     MODEL_NAME,
#     pre_seq_len=NUM_PREFIX_TOKENS,
#     tuning_mode=TUNING_MODE
# ).float().to(DEVICE)
model.config.pad_token_id = tokenizer.pad_token_id
model = nn.DataParallel(model)


"""Let's prepare the SST-2 dataset. We need just one preprocessing function to tokenize the dataset."""

from datasets import load_dataset

# Load your dataset as before
dataset = load_dataset("ctang/util_deontology_rewardtrainer")

# # Use the select method to pick the first 10 rows of each split
# reduced_dataset = {split: original_dataset[split].select(range(10)) for split in original_dataset.keys()}

# dataset = reduced_dataset

task = 'sst2'

# dataset = load_dataset("glue", task)

def preprocess_function(examples):
    return tokenizer(examples["sentence"], padding='max_length', truncation=True, return_token_type_ids=False)

tokenized_datasets = dataset.map(preprocess_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["sentence", "idx", "attention_mask"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

train_dataset = tokenized_datasets["train"].shuffle(seed=SEED)
valid_dataset = tokenized_datasets["validation"].shuffle(seed=SEED)

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)
valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)

"""To monitor training, we need the metric function. For SST-2, the target metric is accuracy. We will load it from the datasets library."""

metric = load_metric('glue', task)

def eval_metrics(model, dataloader, device='cpu'):
    model.eval()
    for batch in dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}

        with torch.no_grad():
            outputs = model(**batch)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])
    model.train()
    return metric.compute()

metric

"""Before setting up optimizers, let's check the model parameters that will be trained."""

for n, p in model.named_parameters():
    if p.requires_grad:
        print(n, p.requires_grad, p.device)

"""The optimizer will only work on **prompts and classifier head**: they are only trainable parameters. Let's initialize the optimizer and the learning rate scheduler."""

optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * NUM_EPOCHS
)

"""Let's initialize wandb for logging and start the training loop!"""

# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'

scaler = torch.cuda.amp.GradScaler()
with open('accuracy.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Epoch', 'Accuracy'])  # Write header

    best_accuracy = 0.0
    epochs_without_improvement = 0
    early_stopping_threshold = 5  # number of epochs to wait for improvement

    for epoch in range(NUM_EPOCHS):
        model.train()

        for batch in tqdm(train_dataloader):
            batch = {k: v.to(DEVICE) for k, v in batch.items()}

            with torch.cuda.amp.autocast():
                outputs = model(**batch)
            loss = outputs.loss
            scaler.scale(loss).backward()

            scaler.step(optimizer)
            scaler.update()
            lr_scheduler.step()
            optimizer.zero_grad()

            # wandb.log({"Train Loss": loss.detach()})
            print("train loss", loss.detach() )

        accuracy = eval_metrics(model, valid_dataloader, device=DEVICE)
        # wandb.log({"Valid Accuracy": accuracy}, commit=False)
        acc = accuracy['accuracy']
        print("accuracy", acc)

        # Write the current epoch and its accuracy to CSV
        writer.writerow([epoch + 1, acc])

        if acc >= best_accuracy:
            best_accuracy = acc
            epochs_without_improvement = 0
            # Save the model if this is your best model so far
            # model.save_pretrained('best_model_directory')
        else:
            epochs_without_improvement += 1

        if epochs_without_improvement >= early_stopping_threshold:
            print("Early stopping triggered")
            break
        if epoch > 0 and epoch % 10 == 0:
            new_model = f"PM-deon-util-epoch-%{epoch}"
            model.push_to_hub(new_model, max_shard_size = "2GB")
            tokenizer.push_to_hub(new_model)

"""Our model has been trained! You can now upload it to the Hub for later use, try out different models [served in the public swarm](https://health.petals.dev/), or [join Petals with your own GPU](https://github.com/bigscience-workshop/petals#connect-your-gpu-and-increase-petals-capacity)!"""


new_model = "PM-deon-util"
model.push_to_hub(new_model, max_shard_size = "2GB")
tokenizer.push_to_hub(new_model)

print("done!")

